当二维数组的规模较大时，按行访问的时间优势非常明显，并且数组规模越大，相对速度越快。但是在数组规模较小时，按行访问和按列访问的时间相差不大。



整型数组存放在内存（也就是DRAM）里，而一旦当程序引用数组里的某个元素a时，系统会将该元素连带紧跟着它后面的若干个元素（构成一个block）一起放入高速缓存区，而高速缓存区通常是SRAM，因此高速缓存区的读写速度要比内存快3到10倍。当程序下一次访问紧跟着元素a的某个元素时，由于该元素在程序访问a时已经存放在了高速缓存区中（也叫做高速缓存命中，hit），因此访问它速度要快于访问a的速度。但是高速缓存区通常较小，如果某一个block长期不被访问，它就会被别的block剔除，这时如果再访问a或者和a同属一个block的元素，程序就无法在高速缓存区里找到该元素（也称高速缓存不命中，miss），此时程序必须到内存中读取该元素，并将其重新放入高速缓存区中，运行速度相对较慢。
  按行访问之所以快，就是因为程序在访问了第一个元素（称为冷不命中，cold miss）之后，接下来的连续多个元素都被存放在了SRAM中，因此接下来的访问都会是命中的，这就大大加快了程序运行的速度。这样的程序很好地利用了高速缓存的局部性，有着较低的不命中率（miss rate），称为是高速缓存友好的（cache friendly）；与之相对的，如果数组规模很大的话，在程序访问了第一个元素之后，第二行的第一个元素并未存入高速缓存，因此按列访问的第二个元素依然是不命中的，更糟糕的是，在不断地遇到不命中之后，之前存入高速缓存的元素很有可能被后面的元素踢出高速缓存区。打个比方，如果在访问第1000行的第一个元素时，正好将第一行第一个元素对应的高速缓存区的block剔除，那么在访问第一行的第二个元素时，又会遇到一次不命中。依此下去，按列遍历整个数组所遭遇的不命中会远多于按行遍历，自然而然其所花的时间会长于按行遍历所花时间。
  如果数组规模比较小，以至于高速缓存区能放得下整个数组，那么block被剔除的情况就不太会发生，按列访问也成功用到了cache的局部性，因此按行访问并没有太大的时间上的优势。不过作为一种良好的习惯，还是推荐大家在写程序时能有局部性的思想，遵从按行访问的原则。
